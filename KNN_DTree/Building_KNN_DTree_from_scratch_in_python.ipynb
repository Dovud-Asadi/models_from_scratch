{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Necessary Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "```\n",
        "\n",
        "### Step 2: Load and Preprocess the Data\n",
        "```python\n",
        "# Load dataset\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.iloc[:, :-1].values  # All columns except the last one\n",
        "y = data.iloc[:, -1].values  # The last column\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "### Step 3: Implement Distance Calculation\n",
        "```python\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "```\n",
        "\n",
        "### Step 4: Implement KNN Algorithm\n",
        "```python\n",
        "def knn_predict(X_train, y_train, X_test, k=3):\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        # Compute distances from x to all training samples\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in X_train]\n",
        "        \n",
        "        # Get the k nearest samples\n",
        "        k_indices = np.argsort(distances)[:k]\n",
        "        k_nearest_labels = [y_train[i] for i in k_indices]\n",
        "        \n",
        "        # Majority vote\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        predictions.append(most_common[0][0])\n",
        "        \n",
        "    return np.array(predictions)\n",
        "```\n",
        "\n",
        "### Step 5: Make Predictions\n",
        "```python\n",
        "k = 3\n",
        "y_pred = knn_predict(X_train, y_train, X_test, k)\n",
        "```\n",
        "\n",
        "### Step 6: Evaluate the Model\n",
        "```python\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "```\n",
        "\n",
        "### Step 7: Plotting the Decision Boundary (2D Example)\n",
        "If you have a dataset with 2 features, you can visualize the decision boundary:\n",
        "```python\n",
        "def plot_decision_boundary(X, y, model, k=3, resolution=0.01):\n",
        "    # Setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    # Plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = model(np.array([xx1.ravel(), xx2.ravel()]).T, k)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # Plot all samples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "                    alpha=0.8, c=colors[idx],\n",
        "                    marker=markers[idx], label=cl,\n",
        "                    edgecolor='black')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('K-Nearest Neighbors Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (only works with 2D data)\n",
        "# plot_decision_boundary(X_train, y_train, lambda x, k: knn_predict(X_train, y_train, x, k), k)\n",
        "```\n",
        "\n",
        "### Additional Improvements and Extensions\n",
        "- **Feature Scaling**: Ensuring all features are on a similar scale is crucial for KNN performance.\n",
        "- **Distance Metrics**: Experiment with different distance metrics, such as Manhattan distance.\n",
        "- **Weighted Voting**: Give more weight to closer neighbors when making predictions.\n",
        "- **Hyperparameter Tuning**: Use cross-validation to find the optimal value of \\( k \\).\n",
        "\n",
        "### Cross-Validation for Hyperparameter Tuning\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def knn_cv(X_train, y_train, k, cv=5):\n",
        "    scores = []\n",
        "    fold_size = len(X_train) // cv\n",
        "    for i in range(cv):\n",
        "        X_valid = X_train[i * fold_size:(i + 1) * fold_size]\n",
        "        y_valid = y_train[i * fold_size:(i + 1) * fold_size]\n",
        "        X_tr = np.concatenate([X_train[:i * fold_size], X_train[(i + 1) * fold_size:]], axis=0)\n",
        "        y_tr = np.concatenate([y_train[:i * fold_size], y_train[(i + 1) * fold_size:]], axis=0)\n",
        "        \n",
        "        y_pred = knn_predict(X_tr, y_tr, X_valid, k)\n",
        "        scores.append(accuracy(y_valid, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Finding the best k value\n",
        "k_values = range(1, 21)\n",
        "cv_scores = [knn_cv(X_train, y_train, k) for k in k_values]\n",
        "\n",
        "# Plotting the cross-validation scores\n",
        "plt.plot(k_values, cv_scores)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('Hyperparameter Tuning for k')\n",
        "plt.show()\n",
        "\n",
        "best_k = k_values[np.argmax(cv_scores)]\n",
        "print(f'Best k: {best_k}')\n",
        "```"
      ],
      "metadata": {
        "id": "EAchql_UVxq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------\n",
        "----------------------------------------------------------\n",
        "-----------------------------------------------------------\n",
        "-----------------------------------------------------------\n",
        "------------------------------------------------------------\n",
        "----------"
      ],
      "metadata": {
        "id": "GZczEnlOWECm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Necessary Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "```\n",
        "\n",
        "### Step 2: Load and Preprocess the Data\n",
        "```python\n",
        "# Load dataset\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.iloc[:, :-1].values  # All columns except the last one\n",
        "y = data.iloc[:, -1].values  # The last column\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "### Step 3: Define the Gini Impurity and Entropy Functions\n",
        "```python\n",
        "def gini(y):\n",
        "    m = len(y)\n",
        "    return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "def entropy(y):\n",
        "    m = len(y)\n",
        "    return -sum((np.sum(y == c) / m) * np.log2(np.sum(y == c) / m) for c in np.unique(y))\n",
        "```\n",
        "\n",
        "### Step 4: Define the Function to Split the Dataset\n",
        "```python\n",
        "def split_dataset(X, y, feature_index, threshold):\n",
        "    left_mask = X[:, feature_index] <= threshold\n",
        "    right_mask = X[:, feature_index] > threshold\n",
        "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
        "```\n",
        "\n",
        "### Step 5: Define the Function to Find the Best Split\n",
        "```python\n",
        "def best_split(X, y, criterion):\n",
        "    best_feature, best_threshold, best_impurity = None, None, float('inf')\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    for feature_index in range(n_features):\n",
        "        thresholds = np.unique(X[:, feature_index])\n",
        "        for threshold in thresholds:\n",
        "            X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "\n",
        "            impurity = (len(y_left) * criterion(y_left) + len(y_right) * criterion(y_right)) / len(y)\n",
        "            if impurity < best_impurity:\n",
        "                best_feature, best_threshold, best_impurity = feature_index, threshold, impurity\n",
        "\n",
        "    return best_feature, best_threshold\n",
        "```\n",
        "\n",
        "### Step 6: Define the Decision Tree Node\n",
        "```python\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "```\n",
        "\n",
        "### Step 7: Build the Decision Tree\n",
        "```python\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2):\n",
        "        self.criterion = gini if criterion == 'gini' else entropy\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "        if (depth >= self.max_depth or num_labels == 1 or num_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature, threshold = best_split(X, y, self.criterion)\n",
        "        if feature is None:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        X_left, X_right, y_left, y_right = split_dataset(X, y, feature, threshold)\n",
        "        left_child = self._grow_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._grow_tree(X_right, y_right, depth + 1)\n",
        "        return Node(feature, threshold, left_child, right_child)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "```\n",
        "\n",
        "### Step 8: Train the Model\n",
        "```python\n",
        "tree = DecisionTree(max_depth=10)\n",
        "tree.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "### Step 9: Make Predictions\n",
        "```python\n",
        "y_pred = tree.predict(X_test)\n",
        "```\n",
        "\n",
        "### Step 10: Evaluate the Model\n",
        "```python\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "```\n",
        "\n",
        "### Step 11: Plot the Decision Tree (Optional)\n",
        "Visualizing decision trees can be complex, especially with high-dimensional data. For 2D data, you can visualize the decision boundaries:\n",
        "```python\n",
        "def plot_decision_boundary(X, y, model, resolution=0.01):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = model.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "                    alpha=0.8, c=colors[idx],\n",
        "                    marker=markers[idx], label=cl,\n",
        "                    edgecolor='black')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('Decision Tree Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (only works with 2D data)\n",
        "# plot_decision_boundary(X_train, y_train, tree)\n",
        "```\n",
        "\n",
        "### Additional Improvements and Extensions\n",
        "- **Pruning**: Implement pruning techniques to avoid overfitting.\n",
        "- **Feature Importance**: Calculate and visualize feature importance.\n",
        "- **Cross-Validation**: Use cross-validation to optimize hyperparameters like `max_depth` and `min_samples_split`.\n",
        "\n",
        "### Cross-Validation for Hyperparameter Tuning\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def cross_validation_accuracy(X, y, criterion='gini', max_depth=None, min_samples_split=2, cv=5):\n",
        "    model = DecisionTree(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Finding the best hyperparameters\n",
        "depths = range(1, 21)\n",
        "cv_scores = [cross_validation_accuracy(X_train, y_train, max_depth=depth) for depth in depths]\n",
        "\n",
        "# Plotting the cross-validation scores\n",
        "plt.plot(depths, cv_scores)\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('Hyperparameter Tuning for Max Depth')\n",
        "plt.show()\n",
        "\n",
        "best_depth = depths[np.argmax(cv_scores)]\n",
        "print(f'Best Max Depth: {best_depth}')\n",
        "```"
      ],
      "metadata": {
        "id": "rd0emi4QWP3e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lT5ipdNfWUNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}